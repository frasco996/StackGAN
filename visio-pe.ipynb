{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"stage = 2","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:38:27.058739Z","iopub.execute_input":"2023-10-12T15:38:27.059058Z","iopub.status.idle":"2023-10-12T15:38:27.067835Z","shell.execute_reply.started":"2023-10-12T15:38:27.059030Z","shell.execute_reply":"2023-10-12T15:38:27.066860Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport pickle\nimport random\nimport time\nimport math\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\ndevice = torch.device(\"cuda\")\nprint(\"cuda to device done\")\n\nimport PIL\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchvision.transforms import ToPILImage\nfrom tqdm import tqdm\n\n\nfrom torchvision.transforms import ToTensor\nfrom tensorboardX import SummaryWriter\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.cuda.amp import autocast, GradScaler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-12T15:38:27.069833Z","iopub.execute_input":"2023-10-12T15:38:27.070742Z","iopub.status.idle":"2023-10-12T15:38:34.375704Z","shell.execute_reply.started":"2023-10-12T15:38:27.070713Z","shell.execute_reply":"2023-10-12T15:38:34.374749Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cuda to device done\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the UpsamplingBlock\nclass UpSamplingBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UpSamplingBlock, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False).to(device)\n        x = self.conv(x)\n        return x\n\n# Define the Stage 1 Generator\nclass Stage1Generator(nn.Module):\n    def __init__(self):\n        super(Stage1Generator, self).__init__()\n        #self.ca_network = CANetwork()\n\n        self.fc2 = nn.Sequential(\n            nn.Linear(128 + 100, 16384, bias=False),\n            nn.ReLU()\n        )\n\n        self.upsample_blocks = nn.ModuleList([\n            UpSamplingBlock(1024, 512),\n            UpSamplingBlock(512, 256),\n            UpSamplingBlock(256, 128),\n            UpSamplingBlock(128, 64)\n        ])\n\n        self.conv_final = nn.Sequential(\n            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, ca, noise):\n        \n        x = torch.cat([ca, noise], dim=1).to(device)\n        x = self.fc2(x)\n        x = x.view(-1, 1024, 4, 4)\n\n        for block in self.upsample_blocks:\n            x = block(x)\n\n        x = self.conv_final(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:38:34.377628Z","iopub.execute_input":"2023-10-12T15:38:34.378463Z","iopub.status.idle":"2023-10-12T15:38:34.388275Z","shell.execute_reply.started":"2023-10-12T15:38:34.378428Z","shell.execute_reply":"2023-10-12T15:38:34.387187Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Define the ConvBlock\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, activation=True):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.fg_activation = activation\n        self.activation = nn.LeakyReLU(0.2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.batch_norm(x)\n        if self.fg_activation:\n            x = self.activation(x)\n        return x\n    \nclass EmbeddingCompressor(nn.Module):\n    def __init__(self):\n        super(EmbeddingCompressor, self).__init__()\n        self.fc = nn.Linear(1024, 128)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc(x)\n        x = self.relu(x)\n        return x\n\n# Define the Stage 1 Discriminator\nclass Stage1Discriminator(nn.Module):\n    def __init__(self):\n        super(Stage1Discriminator, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False)\n        self.leaky_relu1 = nn.LeakyReLU(0.2)\n\n        self.conv_blocks = nn.ModuleList([\n            ConvBlock(64, 128),\n            ConvBlock(128, 256),\n            ConvBlock(256, 512)\n        ])\n\n        self.conv2 = nn.Conv2d(640, 512, kernel_size=1, padding=0, stride=1, bias=False)\n        self.batch_norm2 = nn.BatchNorm2d(512)\n        self.leaky_relu2 = nn.LeakyReLU(0.2)\n\n        self.flatten = nn.Flatten()\n        self.fc = nn.Linear(8192, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, image, mean):\n        x1 = self.conv1(image)\n        x1 = self.leaky_relu1(x1)\n        for block in self.conv_blocks:\n            x1 = block(x1)\n\n        concat = torch.cat((x1, mean), dim=1).to(device)\n        x2 = self.conv2(concat)\n        x2 = self.batch_norm2(x2)\n        x2 = self.leaky_relu2(x2)\n\n        x2 = self.flatten(x2)\n        x2 = self.fc(x2)\n        x2 = self.sigmoid(x2)\n        return x2.view(-1)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:38:34.389716Z","iopub.execute_input":"2023-10-12T15:38:34.390035Z","iopub.status.idle":"2023-10-12T15:38:34.403238Z","shell.execute_reply.started":"2023-10-12T15:38:34.390003Z","shell.execute_reply":"2023-10-12T15:38:34.402371Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def load_class_ids_filenames(class_id_path, filename_path):\n    with open(class_id_path, 'rb') as file:\n        class_id = pickle.load(file, encoding='latin1')\n\n    with open(filename_path, 'rb') as file:\n        filename = pickle.load(file, encoding='latin1')\n\n    return class_id, filename\n\ndef load_text_embeddings(text_embeddings):\n    with open(text_embeddings, 'rb') as file:\n        embeds = pickle.load(file, encoding='latin1')\n        embeds = np.array(embeds)\n    return embeds\n\ndef load_bbox(data_path):\n    bbox_path = os.path.join(data_path, 'bounding_boxes.txt')\n    image_path = os.path.join(data_path, 'images.txt')\n    bbox_df = pd.read_csv(bbox_path, delim_whitespace=True, header=None).astype(int)\n    filename_df = pd.read_csv(image_path, delim_whitespace=True, header=None)\n\n    filenames = filename_df[1].tolist()\n    bbox_dict = {i[:-4]: [] for i in filenames[:2]}\n\n    for i in range(0, len(filenames)):\n        bbox = bbox_df.iloc[i][1:].tolist()\n        dict_key = filenames[i][:-4]\n        bbox_dict[dict_key] = bbox\n\n    return bbox_dict\n\ndef load_images(image_path, bounding_box, size):\n    debug = False\n    \n    image = Image.open(image_path).convert('RGB')\n    w, h = image.size\n    \n    if debug:\n        plt.imshow(image)\n        plt.show()\n    \n    if bounding_box is not None:\n        r = int(np.maximum(bounding_box[2], bounding_box[3]) * 0.75)\n        c_x = int((bounding_box[0] + bounding_box[2]) / 2)\n        c_y = int((bounding_box[1] + bounding_box[3]) / 2)\n        y1 = np.maximum(0, c_y - r)\n        y2 = np.minimum(h, c_y + r)\n        x1 = np.maximum(0, c_x - r)\n        x2 = np.minimum(w, c_x + r)\n        image = image.crop([x1, y1, x2, y2])\n\n    image = image.resize(size, PIL.Image.BILINEAR)\n    \n    image_transform = transforms.Compose([\n            transforms.RandomCrop(size),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n    \n    if debug:\n        plt.imshow(image)\n        plt.show()\n    \n    image = image_transform(image)\n    \n    if debug:\n        x = []\n        x.append(np.array(image))\n        to_pil = ToPILImage()\n        print(x[0].shape)\n        print(x[0])\n        ciao = torch.Tensor(x).to(device)\n        \n        print(ciao.shape)\n        print(ciao)\n        pil_image_real = to_pil(ciao[0].cpu().detach())\n        plt.imshow(pil_image_real)\n        plt.show()\n    \n    return image\n\ndef load_data(filename_path, class_id_path, dataset_path, embeddings_path, size):\n    class_id, filenames = load_class_ids_filenames(class_id_path, filename_path)\n    embeddings = load_text_embeddings(embeddings_path)\n    bbox_dict = load_bbox(dataset_path)\n\n    x, y, embeds = [], [], []\n\n    with tqdm(total=len(filenames), desc=\"Loading Data\") as pbar:\n        for i, filename in enumerate(filenames):\n            bbox = bbox_dict[filename]\n\n            try:\n                image_path = os.path.join(dataset_path, 'images', f'{filename}.jpg')\n                image = load_images(image_path, bbox, size)\n                e = embeddings[i, :, :]\n                embed_index = np.random.randint(0, e.shape[0] - 1)\n                embed = e[embed_index, :]\n\n                x.append(np.array(image))\n                y.append(class_id[i])\n                embeds.append(embed)\n\n            except Exception as e:\n                print(f'{e}')\n\n            # Update the progress bar\n            pbar.update(1)\n\n    x = np.array(x)\n    y = np.array(y)\n    embeds = np.array(embeds)\n\n    return x, y, embeds\n\ndef show_img_batch(image_batch, batch_size, fig_size=(64, 64)):\n    # Calculate the dimensions of the grid (e.g., 2x2 for 4 images)\n    num_rows = int(math.sqrt(batch_size))\n    num_cols = int(math.sqrt(batch_size))\n\n    # Create a larger box (figure) to hold the grid of images\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=fig_size)\n\n    # Flatten the axes array if it's 2D\n    if num_rows == 1 or num_cols == 1:\n        axes = axes.reshape(-1)\n\n    # Loop through the images and display them in the grid\n    for i, img in enumerate(image_batch):\n        row = i // num_cols\n        col = i % num_cols\n        ax = axes[row, col]\n        \n        im = img.data.cpu().numpy()\n        im = (im + 1.0) * 127.5\n        im = im.astype(np.uint8)\n        # print('im', im.shape)\n        im = np.transpose(im, (1, 2, 0))\n        # print('im', im.shape)\n        im = Image.fromarray(im)\n        ax.imshow(im)\n        \n        #to_pil = ToPILImage()\n        #pil_image = to_pil(img.cpu().detach())\n        #ax.imshow(pil_image)\n        \n        ax.axis('off')  # Turn off axis labels and ticks\n\n    # Optionally, adjust spacing between subplots\n    plt.subplots_adjust(wspace=0, hspace=0)\n\n    # Show the grid of images\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:38:34.405690Z","iopub.execute_input":"2023-10-12T15:38:34.406285Z","iopub.status.idle":"2023-10-12T15:38:34.424895Z","shell.execute_reply.started":"2023-10-12T15:38:34.406251Z","shell.execute_reply":"2023-10-12T15:38:34.423997Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"os.makedirs('/kaggle/working/weights', exist_ok=True)\nos.makedirs('/kaggle/working/test', exist_ok=True)\nos.makedirs('/kaggle/working/results_stage2', exist_ok=True)\n\nos.makedirs('/kaggle/working/runs', exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:38:34.426119Z","iopub.execute_input":"2023-10-12T15:38:34.426661Z","iopub.status.idle":"2023-10-12T15:38:34.437872Z","shell.execute_reply.started":"2023-10-12T15:38:34.426630Z","shell.execute_reply":"2023-10-12T15:38:34.437012Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data_dir = \"/kaggle/input/birds2023/birds\"\ntrain_dir = data_dir + \"/train\"\ntest_dir = data_dir + \"/test\"\nembeddings_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\nembeddings_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\nfilename_path_train = train_dir + \"/filenames.pickle\"\nfilename_path_test = test_dir + \"/filenames.pickle\"\nclass_id_path_train = train_dir + \"/class_info.pickle\"\nclass_id_path_test = test_dir + \"/class_info.pickle\"\ndataset_path = \"/kaggle/input/cub2002011/CUB_200_2011\"","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:38:34.439152Z","iopub.execute_input":"2023-10-12T15:38:34.439556Z","iopub.status.idle":"2023-10-12T15:38:34.446506Z","shell.execute_reply.started":"2023-10-12T15:38:34.439513Z","shell.execute_reply":"2023-10-12T15:38:34.445673Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if (stage == 1):\n    x_train, y_train, train_embeds = load_data(filename_path=filename_path_train, class_id_path=class_id_path_train,\n                                 dataset_path=dataset_path, embeddings_path=embeddings_path_train, size=(64, 64))\n    train_dataset = TensorDataset(torch.Tensor(x_train).to(device), torch.Tensor(y_train).to(device), torch.Tensor(train_embeds).to(device))\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n\n    x_test, y_test, test_embeds = load_data(filename_path=filename_path_test, class_id_path=class_id_path_test,\n                                            dataset_path=dataset_path, embeddings_path=embeddings_path_test, size=(64, 64))","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:38:34.447780Z","iopub.execute_input":"2023-10-12T15:38:34.448344Z","iopub.status.idle":"2023-10-12T15:38:34.455254Z","shell.execute_reply.started":"2023-10-12T15:38:34.448298Z","shell.execute_reply":"2023-10-12T15:38:34.454358Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class StackGanStage1(object):\n    def __init__(self, epochs=500, z_dim=100, batch_size=64, stage1_generator_lr=0.0002, stage1_discriminator_lr=0.0002):\n        self.epochs = epochs\n        self.z_dim = z_dim\n        self.batch_size = batch_size\n        self.image_size = 64\n        self.conditioning_dim = 128\n\n        self.embedding_compressor = EmbeddingCompressor().to(device)\n        self.stage1_generator = Stage1Generator().to(device)\n        self.stage1_discriminator = Stage1Discriminator().to(device)\n\n        self.stage1_generator_optimizer = optim.Adam(self.stage1_generator.parameters(), lr=stage1_generator_lr, betas=(0.5, 0.999))\n        self.stage1_discriminator_optimizer = optim.Adam(self.stage1_discriminator.parameters(), lr=stage1_discriminator_lr, betas=(0.5, 0.999))\n\n        self.stage1_generator_optimizer_scheduler = StepLR(self.stage1_generator_optimizer, step_size=20, gamma=0.5)\n        self.stage1_discriminator_optimizer_scheduler = StepLR(self.stage1_discriminator_optimizer, step_size=20, gamma=0.5)\n\n        self.real_label = 0.9  # Define the real label value for the loss function\n        self.fake_label = 0.1  # Define the fake label value for the loss function\n\n        # Create a summary writer for TensorBoard visualization\n        self.writer = SummaryWriter()\n\n    def train_stage1(self):\n\n        for epoch in range(self.epochs):\n            gen_loss = []\n            dis_loss = []\n            num_batches = int(x_train.shape[0] / self.batch_size)\n\n            with tqdm(total=num_batches, desc=f\"Training Epoch {epoch + 1}/{self.epochs}\") as pbar:\n                for batch_idx, (images, labels, embeddings) in enumerate(train_loader):\n                    # Train the discriminator\n                    self.stage1_discriminator_optimizer.zero_grad()\n\n                    real_images = images.to(device)\n\n                    embeddings = embeddings.to(device)\n                    \n                    compressed_embedding = self.embedding_compressor(embeddings).to(device)\n                    \n                    # Generate fake images and conditioning vectors\n                    noise = torch.randn(self.batch_size, self.z_dim).to(device)\n                    fake_images = self.stage1_generator(compressed_embedding, noise)\n                    \n                    compressed_embedding = compressed_embedding.view(-1, 128, 1, 1)\n                    compressed_embedding = compressed_embedding.repeat(1, 1, 4, 4)\n\n                    # Compute discriminator loss for real and fake images\n                    real_labels = (torch.ones(self.batch_size) * 0.9).to(device)\n                    fake_labels = (torch.ones(self.batch_size) * 0.1).to(device)\n                    \n                    real_outputs = self.stage1_discriminator(real_images, compressed_embedding.detach()).to(device)\n                    fake_outputs = self.stage1_discriminator(fake_images.detach(), compressed_embedding.detach()).to(device)\n\n                    real_loss = nn.BCELoss()(real_outputs, real_labels)\n                    fake_loss = nn.BCELoss()(fake_outputs, fake_labels)\n                    discriminator_loss = real_loss + fake_loss\n\n                    discriminator_loss.backward(retain_graph=True)\n                    self.stage1_discriminator_optimizer.step()\n\n                    # Train the generator\n                    self.stage1_generator_optimizer.zero_grad()\n                    \n                    fake_outputs = self.stage1_discriminator(fake_images, compressed_embedding).to(device)\n\n                    generator_loss = nn.BCELoss()(fake_outputs, real_labels)\n\n                    generator_loss.backward(retain_graph=True)\n                    self.stage1_generator_optimizer.step()\n\n                    gen_loss.append(generator_loss.item())\n                    dis_loss.append(discriminator_loss.item())\n\n                    # Update the progress bar\n                    pbar.update(1)\n                print(f\"Epoch {epoch + 1}: Discriminator Loss: {np.mean(dis_loss)}, Generator Loss: {np.mean(gen_loss)}\")\n\n            self.stage1_generator_optimizer_scheduler.step()\n            self.stage1_discriminator_optimizer_scheduler.step()\n\n            # Print and log losses\n            avg_gen_loss = np.mean(gen_loss)\n            avg_dis_loss = np.mean(dis_loss)\n\n            self.writer.add_scalar('Generator Loss', avg_gen_loss, epoch)\n            self.writer.add_scalar('Discriminator Loss', avg_dis_loss, epoch)\n\n            # Save generated images\n            if (epoch + 1) % 5 == 0:\n                with torch.no_grad():\n                    latent_space = torch.randn(self.batch_size, self.z_dim).to(device)\n                    embedding_batch = test_embeds[0 : self.batch_size]\n                    compressed_embedding_batch = self.embedding_compressor(torch.tensor(embedding_batch, dtype=torch.float32).to(device)).to(device)\n                    gen_images = self.stage1_generator(compressed_embedding_batch, latent_space)\n                    \n                    show_img_batch(gen_images, 64, (64, 64))\n                    \n                    #real_images_test = torch.Tensor(x_test).to(device)\n                    #show_img_batch(real_images_test, 64, (64, 64))\n                    \n                    \"\"\"\n                    to_pil = ToPILImage()\n                    for i, image in enumerate(gen_images[:10]):\n                        pil_image = to_pil(image.cpu().detach())\n                        plt.imshow(pil_image)\n                        plt.show()\n                        pil_image.save(f'/kaggle/working/test/gen_1_{epoch + 1}_{i}.jpg')\n                    \"\"\"\n\n            if (epoch + 1) % 5 == 0:\n                torch.save(self.stage1_generator.state_dict(), f'/kaggle/working/weights/stage1_gen_{epoch + 1}.pth')\n                torch.save(self.stage1_discriminator.state_dict(), f'/kaggle/working/weights/stage1_disc_{epoch + 1}.pth')\n                torch.save(self.embedding_compressor.state_dict(), f'/kaggle/working/weights/stage1_embco_{epoch + 1}.pth')","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:38:34.456637Z","iopub.execute_input":"2023-10-12T15:38:34.457205Z","iopub.status.idle":"2023-10-12T15:38:34.474211Z","shell.execute_reply.started":"2023-10-12T15:38:34.457177Z","shell.execute_reply":"2023-10-12T15:38:34.473334Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"if (stage == 1):\n    #stage1 = StackGanStage1(epochs=150)\n    stage1 = StackGanStage1()\n    stage1.train_stage1()","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:38:34.475527Z","iopub.execute_input":"2023-10-12T15:38:34.476084Z","iopub.status.idle":"2023-10-12T15:38:34.486277Z","shell.execute_reply.started":"2023-10-12T15:38:34.476056Z","shell.execute_reply":"2023-10-12T15:38:34.485342Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\n!zip -r file.zip /kaggle/working\nFileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:38:34.489758Z","iopub.execute_input":"2023-10-12T15:38:34.490043Z","iopub.status.idle":"2023-10-12T15:38:35.460547Z","shell.execute_reply.started":"2023-10-12T15:38:34.490008Z","shell.execute_reply":"2023-10-12T15:38:35.459355Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/test/ (stored 0%)\n  adding: kaggle/working/weights/ (stored 0%)\n  adding: kaggle/working/runs/ (stored 0%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/results_stage2/ (stored 0%)\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/file.zip","text/html":"<a href='file.zip' target='_blank'>file.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"class ConcatAlongDims(nn.Module):\n    def forward(self, inputs):\n        c, x = inputs\n        c = c.unsqueeze(2).unsqueeze(3).repeat(1, 1, 16, 16)\n        return torch.cat([c, x], dim=1)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x += input\n        x = self.relu(x)\n        return x\n\nclass Stage2Generator(nn.Module):\n    def __init__(self):\n        super(Stage2Generator, self).__init__()\n        self.conditioning_augmentation = nn.Sequential(\n            nn.Linear(1024, 256),\n            nn.LeakyReLU(0.2)\n        )\n        \n        self.concat_along_dims = ConcatAlongDims()\n        \n        self.downsampling_block = nn.Sequential(\n            nn.ZeroPad2d(1),\n            nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=0, bias=False),\n            nn.ReLU(),\n            nn.ZeroPad2d(1),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=0, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.ZeroPad2d(1),\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU()\n        )\n\n        self.residual_blocks = nn.Sequential(\n            nn.ZeroPad2d(1),\n            nn.Conv2d(640, 512, kernel_size=3, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            ResidualBlock(512, 512),\n            ResidualBlock(512, 512),\n            ResidualBlock(512, 512),\n            ResidualBlock(512, 512)\n        )\n\n        self.upsampling_blocks = nn.ModuleList([\n            UpSamplingBlock(512, 256),\n            UpSamplingBlock(256, 128),\n            UpSamplingBlock(128, 64)\n        ])\n\n        self.final_conv = nn.Conv2d(64, 3, kernel_size=3, padding=1, bias=False)\n        self.tanh = nn.Tanh()\n\n    def forward(self, input_layer1, input_images):\n        \n        x = self.downsampling_block(input_images)\n        concat = self.concat_along_dims((input_layer1, x))\n\n        x = self.residual_blocks(concat)\n\n        for upsample_block in self.upsampling_blocks:\n            x = upsample_block(x)\n\n        x = self.final_conv(x)\n        x = self.tanh(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:38:35.462307Z","iopub.execute_input":"2023-10-12T15:38:35.462762Z","iopub.status.idle":"2023-10-12T15:38:35.476288Z","shell.execute_reply.started":"2023-10-12T15:38:35.462723Z","shell.execute_reply":"2023-10-12T15:38:35.475370Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Create an instance of your model\ngenerator2 = Stage2Generator().to(device)\n\n# Define input tensors with the correct sizes\ninput_layer1 = torch.randn(1, 128).to(device)\ninput_images = torch.randn(1, 3, 64, 64).to(device)\n\n# Pass the inputs through the model to compute shapes\nwith torch.no_grad():\n    output = generator2(input_layer1, input_images)\n\n# Print the architecture of the model\nprint(generator2)\n\n# Print the shape of the output\nprint(\"Output Shape:\", output.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Stage2Discriminator(nn.Module):\n    def __init__(self):\n        super(Stage2Discriminator, self).__init__()\n\n        self.conv1 = nn.Conv2d(3, 128, kernel_size=4, stride=2, padding=1, bias=False)\n        self.leaky_relu = nn.LeakyReLU(0.2)\n        self.conv_blocks = nn.Sequential(\n            ConvBlock(128, 256),\n            ConvBlock(256, 512),\n            ConvBlock(512, 1024),\n            ConvBlock(1024, 512, kernel_size=1, stride=1, padding=0),\n            ConvBlock(512, 256, kernel_size=1, stride=1, padding=0, activation=False),\n        )\n\n        self.conv_block_x1 = nn.Sequential(\n            ConvBlock(256, 64, kernel_size=1, stride=1, padding=0),\n            ConvBlock(64, 64, kernel_size=3, stride=1),\n            ConvBlock(64, 256, kernel_size=3, stride=1, activation=False),\n        )\n\n        self.conv2 = nn.Conv2d(384, 512, kernel_size=1, stride=1, padding=0, bias=False)\n        self.batchnorm = nn.BatchNorm2d(512)\n        \n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(32768, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, input_layer1, mean):\n        x1 = self.conv1(input_layer1)\n        x1 = self.leaky_relu(x1)\n        x1 = self.conv_blocks(x1)\n        \n        x = self.conv_block_x1(x1)\n        \n        x2 = x + x1\n        x2 = self.leaky_relu(x2)\n        \n        concat = torch.cat((x2, mean), dim=1)\n\n        x3 = self.conv2(concat)\n        x3 = self.batchnorm(x3)\n        x3 = self.leaky_relu(x3)\n        x3 = self.fc(x3)\n        return x3.view(-1)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:38:43.772487Z","iopub.execute_input":"2023-10-12T15:38:43.773295Z","iopub.status.idle":"2023-10-12T15:38:43.782637Z","shell.execute_reply.started":"2023-10-12T15:38:43.773263Z","shell.execute_reply":"2023-10-12T15:38:43.781598Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"discriminator2 = Stage2Discriminator().to(device)\n\n# Define input tensors with the correct sizes\nmean = torch.randn(1, 128, 8, 8).to(device)\ninput_images = torch.randn(1, 3, 128, 128).to(device)\n\n# Pass the inputs through the model to compute shapes\nwith torch.no_grad():\n    output = discriminator2(input_images, mean)\n\n# Print the architecture of the model\nprint(generator2)\n\n# Print the shape of the output\nprint(\"Output Shape:\", output.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (stage == 2):\n    x_high_train, y_high_train, high_train_embeds = load_data(filename_path=filename_path_train, class_id_path=class_id_path_train,\n        dataset_path=dataset_path, embeddings_path=embeddings_path_train, size=(128, 128))\n    train_high_dataset = TensorDataset(torch.Tensor(x_high_train), torch.Tensor(y_high_train), torch.Tensor(high_train_embeds))\n    train_high_loader = DataLoader(train_high_dataset, batch_size=64, shuffle=True, drop_last=True)\n\n    x_high_test, y_high_test, high_test_embeds = load_data(filename_path=filename_path_test, class_id_path=class_id_path_test,\n        dataset_path=dataset_path, embeddings_path=embeddings_path_test, size=(128, 128))","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:38:43.968075Z","iopub.execute_input":"2023-10-12T15:38:43.968983Z","iopub.status.idle":"2023-10-12T15:40:43.019079Z","shell.execute_reply.started":"2023-10-12T15:38:43.968951Z","shell.execute_reply":"2023-10-12T15:40:43.018086Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Loading Data: 100%|██████████| 8855/8855 [01:18<00:00, 112.70it/s]\nLoading Data: 100%|██████████| 2933/2933 [00:24<00:00, 119.62it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"class StackGanStage2(object):\n    def __init__(self, epochs=500, z_dim=100, batch_size=64, enable_function=True, stage2_generator_lr=0.0002, stage2_discriminator_lr=0.0002):\n        self.epochs = epochs\n        self.z_dim = z_dim\n        self.enable_function = enable_function\n        \n        self.low_image_size = 64\n        self.high_image_size = 128\n        self.conditioning_dim = 128\n        self.batch_size = batch_size\n        \n        self.stage1_generator = generator.to(device)\n        self.stage2_generator = stage2_gen.to(device)\n        self.embedding_compressor = embedding_compressor.to(device)\n        self.stage2_discriminator = stage2_dis.to(device)\n        self.stage2_generator_optimizer = torch.optim.Adam(self.stage2_generator.parameters(), lr=stage2_generator_lr, betas=(0.5, 0.999))\n        self.stage2_discriminator_optimizer = torch.optim.Adam(self.stage2_discriminator.parameters(), lr=stage2_discriminator_lr, betas=(0.5, 0.999))\n        \n        self.stage2_generator_optimizer_scheduler = StepLR(self.stage2_generator_optimizer, step_size=20, gamma=0.5)\n        self.stage2_discriminator_optimizer_scheduler = StepLR(self.stage2_discriminator_optimizer, step_size=20, gamma=0.5)\n        \n        # Create a summary writer for TensorBoard visualization\n        self.writer = SummaryWriter()\n\n    def train_stage2(self):\n        \n        for epoch in range(self.epochs):\n            \n            torch.cuda.empty_cache()\n            \n            gen_loss = []\n            dis_loss = []\n            num_batches = int(x_high_train.shape[0] / self.batch_size)\n            \n            with tqdm(total=num_batches, desc=f\"Training Epoch {epoch + 1}/{self.epochs}\") as pbar:\n                for batch_idx, (images, labels, embeddings) in enumerate(train_high_loader):\n                    self.stage2_discriminator_optimizer.zero_grad()\n                    \n                    embeddings = embeddings.to(device)\n                    \n                    latent_space = torch.randn(self.batch_size, self.z_dim).to(device)\n\n                    real_images = images.to(device)\n                    \n                    compressed_embedding = self.embedding_compressor(embeddings).to(device)\n\n                    # Generate images\n                    low_res_fakes = self.stage1_generator(compressed_embedding.detach(), latent_space)\n                    high_res_fakes = self.stage2_generator(compressed_embedding.detach(), low_res_fakes.detach().to(device))\n                    high_res_fakes = high_res_fakes.to(device)\n                    \n                    compressed_embedding = compressed_embedding.view(-1, 128, 1, 1)\n                    compressed_embedding = compressed_embedding.repeat(1, 1, 8, 8)\n\n                    # Train discriminator\n                    real = (torch.ones(self.batch_size) * 0.9).to(device)\n                    fake = (torch.ones(self.batch_size) * 0.1).to(device)\n                    \n                    real_output = self.stage2_discriminator(real_images, compressed_embedding.detach()).to(device)\n                    real_loss = nn.BCELoss()(real_output, real)\n                    fake_output = self.stage2_discriminator(high_res_fakes.detach(), compressed_embedding.detach()).to(device)\n                    fake_loss = nn.BCELoss()(fake_output, fake)\n                    d_loss = real_loss + fake_loss\n                    \n                    d_loss.backward(retain_graph=True)\n                    self.stage2_discriminator_optimizer.step()\n                    dis_loss.append(d_loss.item())\n\n                    # Train generator\n                    self.stage2_generator_optimizer.zero_grad()\n    \n                    fake_outputs = self.stage2_discriminator(high_res_fakes, compressed_embedding.detach()).to(device)\n                    generator_loss = nn.BCELoss()(fake_outputs, real)\n                    g_loss = generator_loss\n                    \n                    g_loss.backward(retain_graph=True)\n                    self.stage2_generator_optimizer.step()\n                    gen_loss.append(g_loss.item())\n                    \n                    # Update the progress bar\n                    pbar.update(1)\n                    \n            print(f\"Epoch {epoch + 1}: Discriminator Loss: {np.mean(dis_loss)}, Generator Loss: {np.mean(gen_loss)}\")\n                    \n            self.stage2_generator_optimizer_scheduler.step()\n            self.stage2_discriminator_optimizer_scheduler.step()\n                \n            avg_gen_loss = np.mean(gen_loss)\n            avg_dis_loss = np.mean(dis_loss)\n            self.writer.add_scalar('Generator Loss', avg_gen_loss, epoch)\n            self.writer.add_scalar('Discriminator Loss', avg_dis_loss, epoch)\n\n            # Save generated images\n            if (epoch + 1) % 25 == 0:\n                with torch.no_grad():\n                    latent_space = torch.randn(self.batch_size, self.z_dim).to(device)\n                    embedding_batch = high_test_embeds[0 : self.batch_size]\n                    embedding_tensor = torch.tensor(embedding_batch, dtype=torch.float32).to(device)\n                    embedding_batch_compressed = self.embedding_compressor(embedding_tensor).to(device)\n                    low_fake_images = self.stage1_generator(embedding_batch_compressed, latent_space)\n                    high_fake_images = self.stage2_generator(embedding_batch_compressed, low_fake_images)\n\n                    show_img_batch(low_fake_images, 64, (64, 64))\n                    show_img_batch(high_fake_images, 64, (128, 128))\n                    \n                    \"\"\"\n                    to_pil = ToPILImage()\n                    for i, image in enumerate(high_fake_images[:10]):\n                        pil_image = to_pil(image.cpu().detach())\n                        plt.imshow(pil_image)\n                        plt.show()\n                        pil_image.save(f'/kaggle/working/results_stage2/gen_{epoch + 1}_{i}.png')\n                    \"\"\"\n\n            # Save weights\n            if (epoch + 1) % 5 == 0:\n                torch.save(self.stage2_generator.state_dict(), f'/kaggle/working/results_stage2/stage2_gen_{epoch + 1}.pth')\n                torch.save(self.stage2_discriminator.state_dict(), f'/kaggle/working/results_stage2/stage2_disc_{epoch + 1}.pth')","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:40:43.020923Z","iopub.execute_input":"2023-10-12T15:40:43.021285Z","iopub.status.idle":"2023-10-12T15:40:43.040134Z","shell.execute_reply.started":"2023-10-12T15:40:43.021253Z","shell.execute_reply":"2023-10-12T15:40:43.039114Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"if (stage == 2):\n    generator = Stage1Generator()\n    checkpoint1 = torch.load('/kaggle/input/weights-gan1/stage1_gen_140.pth', map_location=device)\n    generator.load_state_dict(checkpoint1)\n\n    embedding_compressor = EmbeddingCompressor()\n    checkpoint2 = torch.load('/kaggle/input/weights-gan1/stage1_embco_140.pth', map_location=device)\n    embedding_compressor.load_state_dict(checkpoint2)\n\n    stage2_gen = Stage2Generator()\n    stage2_dis = Stage2Discriminator()\n\n    stackgan_stage2 = StackGanStage2(epochs=150)\n    stackgan_stage2.train_stage2()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\n!zip -r file.zip /kaggle/working\nFileLink(r'file.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}