{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pickle\nimport random\nimport time\nimport math\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\ndevice = torch.device(\"cuda\")\nprint(\"cuda to device done\")\n\nimport PIL\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchvision.transforms import ToPILImage\nfrom tqdm import tqdm\n\n\nfrom torchvision.transforms import ToTensor\nfrom tensorboardX import SummaryWriter\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.cuda.amp import autocast, GradScaler\n\nstage = 2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-13T20:25:43.698837Z","iopub.execute_input":"2023-10-13T20:25:43.699159Z","iopub.status.idle":"2023-10-13T20:25:51.966129Z","shell.execute_reply.started":"2023-10-13T20:25:43.699133Z","shell.execute_reply":"2023-10-13T20:25:51.965250Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda to device done\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Stage1 Generator","metadata":{}},{"cell_type":"code","source":"# Define the UpsamplingBlock\nclass UpSamplingBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UpSamplingBlock, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False).to(device)\n        x = self.conv(x)\n        return x\n\n# Define the Stage 1 Generator\nclass Stage1Generator(nn.Module):\n    def __init__(self):\n        super(Stage1Generator, self).__init__()\n        #self.ca_network = CANetwork()\n\n        self.fc2 = nn.Sequential(\n            nn.Linear(128 + 100, 16384, bias=False),\n            nn.ReLU()\n        )\n\n        self.upsample_blocks = nn.ModuleList([\n            UpSamplingBlock(1024, 512),\n            UpSamplingBlock(512, 256),\n            UpSamplingBlock(256, 128),\n            UpSamplingBlock(128, 64)\n        ])\n\n        self.conv_final = nn.Sequential(\n            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, ca, noise):\n        \n        x = torch.cat([ca, noise], dim=1).to(device)\n        x = self.fc2(x)\n        x = x.view(-1, 1024, 4, 4)\n\n        for block in self.upsample_blocks:\n            x = block(x)\n\n        x = self.conv_final(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:25:57.021844Z","iopub.execute_input":"2023-10-13T20:25:57.022165Z","iopub.status.idle":"2023-10-13T20:25:57.031145Z","shell.execute_reply.started":"2023-10-13T20:25:57.022139Z","shell.execute_reply":"2023-10-13T20:25:57.029724Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Stage1 Discriminator","metadata":{}},{"cell_type":"code","source":"# Define the ConvBlock\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, activation=True):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        self.fg_activation = activation\n        self.activation = nn.LeakyReLU(0.2)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.batch_norm(x)\n        if self.fg_activation:\n            x = self.activation(x)\n        return x\n    \nclass EmbeddingCompressor(nn.Module):\n    def __init__(self):\n        super(EmbeddingCompressor, self).__init__()\n        self.fc = nn.Linear(1024, 128)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc(x)\n        x = self.relu(x)\n        return x\n\n# Define the Stage 1 Discriminator\nclass Stage1Discriminator(nn.Module):\n    def __init__(self):\n        super(Stage1Discriminator, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False)\n        self.leaky_relu1 = nn.LeakyReLU(0.2)\n\n        self.conv_blocks = nn.ModuleList([\n            ConvBlock(64, 128),\n            ConvBlock(128, 256),\n            ConvBlock(256, 512)\n        ])\n\n        self.conv2 = nn.Conv2d(640, 512, kernel_size=1, padding=0, stride=1, bias=False)\n        self.batch_norm2 = nn.BatchNorm2d(512)\n        self.leaky_relu2 = nn.LeakyReLU(0.2)\n\n        self.flatten = nn.Flatten()\n        self.fc = nn.Linear(8192, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, image, mean):\n        x1 = self.conv1(image)\n        x1 = self.leaky_relu1(x1)\n        for block in self.conv_blocks:\n            x1 = block(x1)\n\n        concat = torch.cat((x1, mean), dim=1).to(device)\n        x2 = self.conv2(concat)\n        x2 = self.batch_norm2(x2)\n        x2 = self.leaky_relu2(x2)\n\n        x2 = self.flatten(x2)\n        x2 = self.fc(x2)\n        x2 = self.sigmoid(x2)\n        return x2.view(-1)","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:26:00.220913Z","iopub.execute_input":"2023-10-13T20:26:00.221277Z","iopub.status.idle":"2023-10-13T20:26:00.232419Z","shell.execute_reply.started":"2023-10-13T20:26:00.221249Z","shell.execute_reply":"2023-10-13T20:26:00.231360Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset 64x64","metadata":{}},{"cell_type":"code","source":"def load_class_ids_filenames(class_id_path, filename_path):\n    with open(class_id_path, 'rb') as file:\n        class_id = pickle.load(file, encoding='latin1')\n\n    with open(filename_path, 'rb') as file:\n        filename = pickle.load(file, encoding='latin1')\n\n    return class_id, filename\n\ndef load_text_embeddings(text_embeddings):\n    with open(text_embeddings, 'rb') as file:\n        embeds = pickle.load(file, encoding='latin1')\n        embeds = np.array(embeds)\n    return embeds\n\ndef load_bbox(data_path):\n    bbox_path = os.path.join(data_path, 'bounding_boxes.txt')\n    image_path = os.path.join(data_path, 'images.txt')\n    bbox_df = pd.read_csv(bbox_path, delim_whitespace=True, header=None).astype(int)\n    filename_df = pd.read_csv(image_path, delim_whitespace=True, header=None)\n\n    filenames = filename_df[1].tolist()\n    bbox_dict = {i[:-4]: [] for i in filenames[:2]}\n\n    for i in range(0, len(filenames)):\n        bbox = bbox_df.iloc[i][1:].tolist()\n        dict_key = filenames[i][:-4]\n        bbox_dict[dict_key] = bbox\n\n    return bbox_dict\n\ndef load_images(image_path, bounding_box, size):\n    debug = False\n    \n    image = Image.open(image_path).convert('RGB')\n    w, h = image.size\n    \n    if debug:\n        plt.imshow(image)\n        plt.show()\n    \n    if bounding_box is not None:\n        r = int(np.maximum(bounding_box[2], bounding_box[3]) * 0.75)\n        c_x = int((bounding_box[0] + bounding_box[2]) / 2)\n        c_y = int((bounding_box[1] + bounding_box[3]) / 2)\n        y1 = np.maximum(0, c_y - r)\n        y2 = np.minimum(h, c_y + r)\n        x1 = np.maximum(0, c_x - r)\n        x2 = np.minimum(w, c_x + r)\n        image = image.crop([x1, y1, x2, y2])\n\n    image = image.resize(size, PIL.Image.BILINEAR)\n    \n    image_transform = transforms.Compose([\n            transforms.RandomCrop(size),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n    \n    if debug:\n        plt.imshow(image)\n        plt.show()\n    \n    image = image_transform(image)\n    \n    if debug:\n        x = []\n        x.append(np.array(image))\n        to_pil = ToPILImage()\n        print(x[0].shape)\n        print(x[0])\n        ciao = torch.Tensor(x).to(device)\n        \n        print(ciao.shape)\n        print(ciao)\n        pil_image_real = to_pil(ciao[0].cpu().detach())\n        plt.imshow(pil_image_real)\n        plt.show()\n    \n    return image\n\ndef load_data(filename_path, class_id_path, dataset_path, embeddings_path, size):\n    class_id, filenames = load_class_ids_filenames(class_id_path, filename_path)\n    embeddings = load_text_embeddings(embeddings_path)\n    bbox_dict = load_bbox(dataset_path)\n\n    x, y, embeds = [], [], []\n\n    with tqdm(total=len(filenames), desc=\"Loading Data\") as pbar:\n        for i, filename in enumerate(filenames):\n            bbox = bbox_dict[filename]\n\n            try:\n                image_path = os.path.join(dataset_path, 'images', f'{filename}.jpg')\n                image = load_images(image_path, bbox, size)\n                e = embeddings[i, :, :]\n                embed_index = np.random.randint(0, e.shape[0] - 1)\n                embed = e[embed_index, :]\n\n                x.append(np.array(image))\n                y.append(class_id[i])\n                embeds.append(embed)\n\n            except Exception as e:\n                print(f'{e}')\n\n            # Update the progress bar\n            pbar.update(1)\n\n    x = np.array(x)\n    y = np.array(y)\n    embeds = np.array(embeds)\n\n    return x, y, embeds\n\ndef show_img_batch(image_batch, batch_size, fig_size=(64, 64)):\n    # Calculate the dimensions of the grid (e.g., 2x2 for 4 images)\n    num_rows = int(math.sqrt(batch_size))\n    num_cols = int(math.sqrt(batch_size))\n\n    # Create a larger box (figure) to hold the grid of images\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=fig_size)\n\n    # Flatten the axes array if it's 2D\n    if num_rows == 1 or num_cols == 1:\n        axes = axes.reshape(-1)\n\n    # Loop through the images and display them in the grid\n    for i, img in enumerate(image_batch):\n        row = i // num_cols\n        col = i % num_cols\n        ax = axes[row, col]\n        \n        im = img.data.cpu().numpy()\n        im = (im + 1.0) * 127.5\n        im = im.astype(np.uint8)\n        # print('im', im.shape)\n        im = np.transpose(im, (1, 2, 0))\n        # print('im', im.shape)\n        im = Image.fromarray(im)\n        ax.imshow(im)\n        \n        #to_pil = ToPILImage()\n        #pil_image = to_pil(img.cpu().detach())\n        #ax.imshow(pil_image)\n        \n        ax.axis('off')  # Turn off axis labels and ticks\n\n    # Optionally, adjust spacing between subplots\n    plt.subplots_adjust(wspace=0, hspace=0)\n\n    # Show the grid of images\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:26:02.331003Z","iopub.execute_input":"2023-10-13T20:26:02.331357Z","iopub.status.idle":"2023-10-13T20:26:02.352087Z","shell.execute_reply.started":"2023-10-13T20:26:02.331329Z","shell.execute_reply":"2023-10-13T20:26:02.351127Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"os.makedirs('/kaggle/working/weights', exist_ok=True)\nos.makedirs('/kaggle/working/test', exist_ok=True)\nos.makedirs('/kaggle/working/results_stage2', exist_ok=True)\n\nos.makedirs('/kaggle/working/runs', exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:26:07.460250Z","iopub.execute_input":"2023-10-13T20:26:07.460597Z","iopub.status.idle":"2023-10-13T20:26:07.466154Z","shell.execute_reply.started":"2023-10-13T20:26:07.460567Z","shell.execute_reply":"2023-10-13T20:26:07.464950Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data_dir = \"/kaggle/input/birdss/birds\"\ntrain_dir = data_dir + \"/train\"\ntest_dir = data_dir + \"/test\"\nembeddings_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\nembeddings_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\nfilename_path_train = train_dir + \"/filenames.pickle\"\nfilename_path_test = test_dir + \"/filenames.pickle\"\nclass_id_path_train = train_dir + \"/class_info.pickle\"\nclass_id_path_test = test_dir + \"/class_info.pickle\"\ndataset_path = \"/kaggle/input/cub2002011/CUB_200_2011\"","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:26:09.430853Z","iopub.execute_input":"2023-10-13T20:26:09.431199Z","iopub.status.idle":"2023-10-13T20:26:09.436606Z","shell.execute_reply.started":"2023-10-13T20:26:09.431172Z","shell.execute_reply":"2023-10-13T20:26:09.435518Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"if (stage == 1):\n    x_train, y_train, train_embeds = load_data(filename_path=filename_path_train, class_id_path=class_id_path_train,\n                                 dataset_path=dataset_path, embeddings_path=embeddings_path_train, size=(64, 64))\n    train_dataset = TensorDataset(torch.Tensor(x_train).to(device), torch.Tensor(y_train).to(device), torch.Tensor(train_embeds).to(device))\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n\n    x_test, y_test, test_embeds = load_data(filename_path=filename_path_test, class_id_path=class_id_path_test,\n                                            dataset_path=dataset_path, embeddings_path=embeddings_path_test, size=(64, 64))","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:26:11.539220Z","iopub.execute_input":"2023-10-13T20:26:11.539693Z","iopub.status.idle":"2023-10-13T20:29:08.168650Z","shell.execute_reply.started":"2023-10-13T20:26:11.539609Z","shell.execute_reply":"2023-10-13T20:29:08.167652Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Loading Data: 100%|██████████| 8855/8855 [01:59<00:00, 74.16it/s]\nLoading Data: 100%|██████████| 2933/2933 [00:38<00:00, 75.33it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Stage1 Training","metadata":{}},{"cell_type":"code","source":"class StackGanStage1(object):\n    def __init__(self, epochs=500, z_dim=100, batch_size=64, stage1_generator_lr=0.0002, stage1_discriminator_lr=0.0002):\n        self.epochs = epochs\n        self.z_dim = z_dim\n        self.batch_size = batch_size\n        self.image_size = 64\n        self.conditioning_dim = 128\n\n        self.embedding_compressor = EmbeddingCompressor().to(device)\n        self.stage1_generator = Stage1Generator().to(device)\n        self.stage1_discriminator = Stage1Discriminator().to(device)\n\n        self.stage1_generator_optimizer = optim.Adam(self.stage1_generator.parameters(), lr=stage1_generator_lr, betas=(0.5, 0.999))\n        self.stage1_discriminator_optimizer = optim.Adam(self.stage1_discriminator.parameters(), lr=stage1_discriminator_lr, betas=(0.5, 0.999))\n\n        self.stage1_generator_optimizer_scheduler = StepLR(self.stage1_generator_optimizer, step_size=20, gamma=0.5)\n        self.stage1_discriminator_optimizer_scheduler = StepLR(self.stage1_discriminator_optimizer, step_size=20, gamma=0.5)\n\n        self.real_label = 0.9  # Define the real label value for the loss function\n        self.fake_label = 0.1  # Define the fake label value for the loss function\n\n        # Create a summary writer for TensorBoard visualization\n        self.writer = SummaryWriter()\n\n    def train_stage1(self):\n\n        for epoch in range(self.epochs):\n            gen_loss = []\n            dis_loss = []\n            num_batches = int(x_train.shape[0] / self.batch_size)\n\n            with tqdm(total=num_batches, desc=f\"Training Epoch {epoch + 1}/{self.epochs}\") as pbar:\n                for batch_idx, (images, labels, embeddings) in enumerate(train_loader):\n                    # Train the discriminator\n                    self.stage1_discriminator_optimizer.zero_grad()\n\n                    real_images = images.to(device)\n\n                    embeddings = embeddings.to(device)\n                    \n                    compressed_embedding = self.embedding_compressor(embeddings).to(device)\n                    \n                    # Generate fake images and conditioning vectors\n                    noise = torch.randn(self.batch_size, self.z_dim).to(device)\n                    fake_images = self.stage1_generator(compressed_embedding, noise)\n                    \n                    compressed_embedding = compressed_embedding.view(-1, 128, 1, 1)\n                    compressed_embedding = compressed_embedding.repeat(1, 1, 4, 4)\n\n                    # Compute discriminator loss for real and fake images\n                    real_labels = (torch.ones(self.batch_size) * 0.9).to(device)\n                    fake_labels = (torch.ones(self.batch_size) * 0.1).to(device)\n                    \n                    real_outputs = self.stage1_discriminator(real_images, compressed_embedding.detach()).to(device)\n                    fake_outputs = self.stage1_discriminator(fake_images.detach(), compressed_embedding.detach()).to(device)\n\n                    real_loss = nn.BCELoss()(real_outputs, real_labels)\n                    fake_loss = nn.BCELoss()(fake_outputs, fake_labels)\n                    discriminator_loss = real_loss + fake_loss\n\n                    discriminator_loss.backward(retain_graph=True)\n                    self.stage1_discriminator_optimizer.step()\n\n                    # Train the generator\n                    self.stage1_generator_optimizer.zero_grad()\n                    \n                    fake_outputs = self.stage1_discriminator(fake_images, compressed_embedding).to(device)\n\n                    generator_loss = nn.BCELoss()(fake_outputs, real_labels)\n\n                    generator_loss.backward(retain_graph=True)\n                    self.stage1_generator_optimizer.step()\n\n                    gen_loss.append(generator_loss.item())\n                    dis_loss.append(discriminator_loss.item())\n\n                    # Update the progress bar\n                    pbar.update(1)\n                print(f\"Epoch {epoch + 1}: Discriminator Loss: {np.mean(dis_loss)}, Generator Loss: {np.mean(gen_loss)}\")\n\n            self.stage1_generator_optimizer_scheduler.step()\n            self.stage1_discriminator_optimizer_scheduler.step()\n\n            # Print and log losses\n            avg_gen_loss = np.mean(gen_loss)\n            avg_dis_loss = np.mean(dis_loss)\n\n            self.writer.add_scalar('Generator Loss', avg_gen_loss, epoch)\n            self.writer.add_scalar('Discriminator Loss', avg_dis_loss, epoch)\n\n            # Save generated images\n            if (epoch + 1) % 5 == 0:\n                with torch.no_grad():\n                    latent_space = torch.randn(self.batch_size, self.z_dim).to(device)\n                    embedding_batch = test_embeds[0 : self.batch_size]\n                    compressed_embedding_batch = self.embedding_compressor(torch.tensor(embedding_batch, dtype=torch.float32).to(device)).to(device)\n                    gen_images = self.stage1_generator(compressed_embedding_batch, latent_space)\n                    \n                    show_img_batch(gen_images, 64, (64, 64))\n                    \n                    #real_images_test = torch.Tensor(x_test).to(device)\n                    #show_img_batch(real_images_test, 64, (64, 64))\n                    \n                    \"\"\"\n                    to_pil = ToPILImage()\n                    for i, image in enumerate(gen_images[:10]):\n                        pil_image = to_pil(image.cpu().detach())\n                        plt.imshow(pil_image)\n                        plt.show()\n                        pil_image.save(f'/kaggle/working/test/gen_1_{epoch + 1}_{i}.jpg')\n                    \"\"\"\n\n            if (epoch + 1) % 5 == 0:\n                torch.save(self.stage1_generator.state_dict(), f'/kaggle/working/weights/stage1_gen_{epoch + 1}.pth')\n                torch.save(self.stage1_discriminator.state_dict(), f'/kaggle/working/weights/stage1_disc_{epoch + 1}.pth')\n                torch.save(self.embedding_compressor.state_dict(), f'/kaggle/working/weights/stage1_embco_{epoch + 1}.pth')","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:29:12.160534Z","iopub.execute_input":"2023-10-13T20:29:12.160946Z","iopub.status.idle":"2023-10-13T20:29:12.179111Z","shell.execute_reply.started":"2023-10-13T20:29:12.160917Z","shell.execute_reply":"2023-10-13T20:29:12.177891Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"if (stage == 1):\n    #stage1 = StackGanStage1(epochs=150)\n    stage1 = StackGanStage1()\n    stage1.train_stage1()","metadata":{"execution":{"iopub.status.busy":"2023-10-12T15:38:34.475527Z","iopub.execute_input":"2023-10-12T15:38:34.476084Z","iopub.status.idle":"2023-10-12T15:38:34.486277Z","shell.execute_reply.started":"2023-10-12T15:38:34.476056Z","shell.execute_reply":"2023-10-12T15:38:34.485342Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Stage2 Generator","metadata":{}},{"cell_type":"code","source":"class ConcatAlongDims(nn.Module):\n    def forward(self, inputs):\n        c, x = inputs\n        c = c.unsqueeze(2).unsqueeze(3).repeat(1, 1, 16, 16)\n        return torch.cat([c, x], dim=1)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x += input\n        x = self.relu(x)\n        return x\n\nclass Stage2Generator(nn.Module):\n    def __init__(self):\n        super(Stage2Generator, self).__init__()\n        self.conditioning_augmentation = nn.Sequential(\n            nn.Linear(1024, 256),\n            nn.LeakyReLU(0.2)\n        )\n        \n        self.concat_along_dims = ConcatAlongDims()\n        \n        self.downsampling_block = nn.Sequential(\n            nn.ZeroPad2d(1),\n            nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=0, bias=False),\n            nn.ReLU(),\n            nn.ZeroPad2d(1),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=0, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.ZeroPad2d(1),\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU()\n        )\n\n        self.residual_blocks = nn.Sequential(\n            nn.ZeroPad2d(1),\n            nn.Conv2d(640, 512, kernel_size=3, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            ResidualBlock(512, 512),\n            ResidualBlock(512, 512),\n            ResidualBlock(512, 512),\n            ResidualBlock(512, 512)\n        )\n\n        self.upsampling_blocks = nn.ModuleList([\n            UpSamplingBlock(512, 256),\n            UpSamplingBlock(256, 128),\n            UpSamplingBlock(128, 64)\n        ])\n\n        self.final_conv = nn.Conv2d(64, 3, kernel_size=3, padding=1, bias=False)\n        self.tanh = nn.Tanh()\n\n    def forward(self, input_layer1, input_images):\n        \n        x = self.downsampling_block(input_images)\n        concat = self.concat_along_dims((input_layer1, x))\n\n        x = self.residual_blocks(concat)\n\n        for upsample_block in self.upsampling_blocks:\n            x = upsample_block(x)\n\n        x = self.final_conv(x)\n        x = self.tanh(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:39:27.931060Z","iopub.execute_input":"2023-10-13T20:39:27.931409Z","iopub.status.idle":"2023-10-13T20:39:27.946023Z","shell.execute_reply.started":"2023-10-13T20:39:27.931382Z","shell.execute_reply":"2023-10-13T20:39:27.944806Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Create an instance of your model\ngenerator2 = Stage2Generator().to(device)\n\n# Define input tensors with the correct sizes\ninput_layer1 = torch.randn(1, 128).to(device)\ninput_images = torch.randn(1, 3, 64, 64).to(device)\n\n# Pass the inputs through the model to compute shapes\nwith torch.no_grad():\n    output = generator2(input_layer1, input_images)\n\n# Print the architecture of the model\nprint(generator2)\n\n# Print the shape of the output\nprint(\"Output Shape:\", output.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:39:31.231565Z","iopub.execute_input":"2023-10-13T20:39:31.231941Z","iopub.status.idle":"2023-10-13T20:39:31.503822Z","shell.execute_reply.started":"2023-10-13T20:39:31.231914Z","shell.execute_reply":"2023-10-13T20:39:31.502919Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Stage2Generator(\n  (conditioning_augmentation): Sequential(\n    (0): Linear(in_features=1024, out_features=256, bias=True)\n    (1): LeakyReLU(negative_slope=0.2)\n  )\n  (concat_along_dims): ConcatAlongDims()\n  (downsampling_block): Sequential(\n    (0): ZeroPad2d((1, 1, 1, 1))\n    (1): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (2): ReLU()\n    (3): ZeroPad2d((1, 1, 1, 1))\n    (4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), bias=False)\n    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): ZeroPad2d((1, 1, 1, 1))\n    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), bias=False)\n    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): ReLU()\n  )\n  (residual_blocks): Sequential(\n    (0): ZeroPad2d((1, 1, 1, 1))\n    (1): Conv2d(640, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): ReLU()\n    (4): ResidualBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (5): ResidualBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (6): ResidualBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (7): ResidualBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (upsampling_blocks): ModuleList(\n    (0): UpSamplingBlock(\n      (conv): Sequential(\n        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n    )\n    (1): UpSamplingBlock(\n      (conv): Sequential(\n        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n    )\n    (2): UpSamplingBlock(\n      (conv): Sequential(\n        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n    )\n  )\n  (final_conv): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (tanh): Tanh()\n)\nOutput Shape: torch.Size([1, 3, 128, 128])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Stage2 Discriminator","metadata":{}},{"cell_type":"code","source":"class Stage2Discriminator(nn.Module):\n    def __init__(self):\n        super(Stage2Discriminator, self).__init__()\n\n        self.conv1 = nn.Conv2d(3, 128, kernel_size=4, stride=2, padding=1, bias=False)\n        self.leaky_relu = nn.LeakyReLU(0.2)\n        self.conv_blocks = nn.Sequential(\n            ConvBlock(128, 256),\n            ConvBlock(256, 512),\n            ConvBlock(512, 1024),\n            ConvBlock(1024, 512, kernel_size=1, stride=1, padding=0),\n            ConvBlock(512, 256, kernel_size=1, stride=1, padding=0, activation=False),\n        )\n\n        self.conv_block_x1 = nn.Sequential(\n            ConvBlock(256, 64, kernel_size=1, stride=1, padding=0),\n            ConvBlock(64, 64, kernel_size=3, stride=1),\n            ConvBlock(64, 256, kernel_size=3, stride=1, activation=False),\n        )\n\n        self.conv2 = nn.Conv2d(384, 512, kernel_size=1, stride=1, padding=0, bias=False)\n        self.batchnorm = nn.BatchNorm2d(512)\n        \n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(32768, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, input_layer1, mean):\n        x1 = self.conv1(input_layer1)\n        x1 = self.leaky_relu(x1)\n        x1 = self.conv_blocks(x1)\n        \n        x = self.conv_block_x1(x1)\n        \n        x2 = x + x1\n        x2 = self.leaky_relu(x2)\n        \n        concat = torch.cat((x2, mean), dim=1)\n\n        x3 = self.conv2(concat)\n        x3 = self.batchnorm(x3)\n        x3 = self.leaky_relu(x3)\n        x3 = self.fc(x3)\n        return x3.view(-1)","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:39:34.851293Z","iopub.execute_input":"2023-10-13T20:39:34.851622Z","iopub.status.idle":"2023-10-13T20:39:34.861104Z","shell.execute_reply.started":"2023-10-13T20:39:34.851593Z","shell.execute_reply":"2023-10-13T20:39:34.859862Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"discriminator2 = Stage2Discriminator().to(device)\n\n# Define input tensors with the correct sizes\nmean = torch.randn(1, 128, 8, 8).to(device)\ninput_images = torch.randn(1, 3, 128, 128).to(device)\n\n# Pass the inputs through the model to compute shapes\nwith torch.no_grad():\n    output = discriminator2(input_images, mean)\n\n# Print the architecture of the model\nprint(generator2)\n\n# Print the shape of the output\nprint(\"Output Shape:\", output.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:39:38.320195Z","iopub.execute_input":"2023-10-13T20:39:38.320515Z","iopub.status.idle":"2023-10-13T20:39:38.473445Z","shell.execute_reply.started":"2023-10-13T20:39:38.320489Z","shell.execute_reply":"2023-10-13T20:39:38.472465Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Stage2Generator(\n  (conditioning_augmentation): Sequential(\n    (0): Linear(in_features=1024, out_features=256, bias=True)\n    (1): LeakyReLU(negative_slope=0.2)\n  )\n  (concat_along_dims): ConcatAlongDims()\n  (downsampling_block): Sequential(\n    (0): ZeroPad2d((1, 1, 1, 1))\n    (1): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (2): ReLU()\n    (3): ZeroPad2d((1, 1, 1, 1))\n    (4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), bias=False)\n    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU()\n    (7): ZeroPad2d((1, 1, 1, 1))\n    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), bias=False)\n    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): ReLU()\n  )\n  (residual_blocks): Sequential(\n    (0): ZeroPad2d((1, 1, 1, 1))\n    (1): Conv2d(640, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): ReLU()\n    (4): ResidualBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (5): ResidualBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (6): ResidualBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (7): ResidualBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (upsampling_blocks): ModuleList(\n    (0): UpSamplingBlock(\n      (conv): Sequential(\n        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n    )\n    (1): UpSamplingBlock(\n      (conv): Sequential(\n        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n    )\n    (2): UpSamplingBlock(\n      (conv): Sequential(\n        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n      )\n    )\n  )\n  (final_conv): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (tanh): Tanh()\n)\nOutput Shape: torch.Size([1])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#  Load Dataset 128x128","metadata":{}},{"cell_type":"code","source":"if (stage == 2):\n    x_high_train, y_high_train, high_train_embeds = load_data(filename_path=filename_path_train, class_id_path=class_id_path_train,\n        dataset_path=dataset_path, embeddings_path=embeddings_path_train, size=(128, 128))\n    train_high_dataset = TensorDataset(torch.Tensor(x_high_train), torch.Tensor(y_high_train), torch.Tensor(high_train_embeds))\n    train_high_loader = DataLoader(train_high_dataset, batch_size=64, shuffle=True, drop_last=True)\n\n    x_high_test, y_high_test, high_test_embeds = load_data(filename_path=filename_path_test, class_id_path=class_id_path_test,\n        dataset_path=dataset_path, embeddings_path=embeddings_path_test, size=(128, 128))","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:39:41.110877Z","iopub.execute_input":"2023-10-13T20:39:41.111199Z","iopub.status.idle":"2023-10-13T20:40:54.034409Z","shell.execute_reply.started":"2023-10-13T20:39:41.111173Z","shell.execute_reply":"2023-10-13T20:40:54.033433Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Loading Data: 100%|██████████| 8855/8855 [00:47<00:00, 185.72it/s]\nLoading Data: 100%|██████████| 2933/2933 [00:15<00:00, 184.30it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Stage2 Training","metadata":{}},{"cell_type":"code","source":"class StackGanStage2(object):\n    def __init__(self, epochs=500, z_dim=100, batch_size=64, enable_function=True, stage2_generator_lr=0.0002, stage2_discriminator_lr=0.0002):\n        self.epochs = epochs\n        self.z_dim = z_dim\n        self.enable_function = enable_function\n        \n        self.low_image_size = 64\n        self.high_image_size = 128\n        self.conditioning_dim = 128\n        self.batch_size = batch_size\n        \n        self.stage1_generator = generator.to(device)\n        self.stage2_generator = stage2_gen.to(device)\n        self.embedding_compressor = embedding_compressor.to(device)\n        self.stage2_discriminator = stage2_dis.to(device)\n        self.stage2_generator_optimizer = torch.optim.Adam(self.stage2_generator.parameters(), lr=stage2_generator_lr, betas=(0.5, 0.999))\n        self.stage2_discriminator_optimizer = torch.optim.Adam(self.stage2_discriminator.parameters(), lr=stage2_discriminator_lr, betas=(0.5, 0.999))\n        \n        self.stage2_generator_optimizer_scheduler = StepLR(self.stage2_generator_optimizer, step_size=20, gamma=0.5)\n        self.stage2_discriminator_optimizer_scheduler = StepLR(self.stage2_discriminator_optimizer, step_size=20, gamma=0.5)\n        \n        # Create a summary writer for TensorBoard visualization\n        self.writer = SummaryWriter()\n\n    def train_stage2(self):\n        \n        for epoch in range(self.epochs):\n            \n            torch.cuda.empty_cache()\n            \n            gen_loss = []\n            dis_loss = []\n            num_batches = int(x_high_train.shape[0] / self.batch_size)\n            \n            with tqdm(total=num_batches, desc=f\"Training Epoch {epoch + 1}/{self.epochs}\") as pbar:\n                for batch_idx, (images, labels, embeddings) in enumerate(train_high_loader):\n                    self.stage2_discriminator_optimizer.zero_grad()\n                    \n                    embeddings = embeddings.to(device)\n                    \n                    latent_space = torch.randn(self.batch_size, self.z_dim).to(device)\n\n                    real_images = images.to(device)\n                    \n                    compressed_embedding = self.embedding_compressor(embeddings).to(device)\n\n                    # Generate images\n                    low_res_fakes = self.stage1_generator(compressed_embedding.detach(), latent_space)\n                    high_res_fakes = self.stage2_generator(compressed_embedding.detach(), low_res_fakes.detach().to(device))\n                    high_res_fakes = high_res_fakes.to(device)\n                    \n                    compressed_embedding = compressed_embedding.view(-1, 128, 1, 1)\n                    compressed_embedding = compressed_embedding.repeat(1, 1, 8, 8)\n\n                    # Train discriminator\n                    real = (torch.ones(self.batch_size) * 0.9).to(device)\n                    fake = (torch.ones(self.batch_size) * 0.1).to(device)\n                    \n                    real_output = self.stage2_discriminator(real_images, compressed_embedding.detach()).to(device)\n                    real_loss = nn.BCELoss()(real_output, real)\n                    fake_output = self.stage2_discriminator(high_res_fakes.detach(), compressed_embedding.detach()).to(device)\n                    fake_loss = nn.BCELoss()(fake_output, fake)\n                    d_loss = real_loss + fake_loss\n                    \n                    d_loss.backward(retain_graph=True)\n                    self.stage2_discriminator_optimizer.step()\n                    dis_loss.append(d_loss.item())\n\n                    # Train generator\n                    self.stage2_generator_optimizer.zero_grad()\n    \n                    fake_outputs = self.stage2_discriminator(high_res_fakes, compressed_embedding.detach()).to(device)\n                    generator_loss = nn.BCELoss()(fake_outputs, real)\n                    g_loss = generator_loss\n                    \n                    g_loss.backward(retain_graph=True)\n                    self.stage2_generator_optimizer.step()\n                    gen_loss.append(g_loss.item())\n                    \n                    # Update the progress bar\n                    pbar.update(1)\n                    \n            print(f\"Epoch {epoch + 1}: Discriminator Loss: {np.mean(dis_loss)}, Generator Loss: {np.mean(gen_loss)}\")\n                    \n            self.stage2_generator_optimizer_scheduler.step()\n            self.stage2_discriminator_optimizer_scheduler.step()\n                \n            avg_gen_loss = np.mean(gen_loss)\n            avg_dis_loss = np.mean(dis_loss)\n            self.writer.add_scalar('Generator Loss', avg_gen_loss, epoch)\n            self.writer.add_scalar('Discriminator Loss', avg_dis_loss, epoch)\n\n            # Save generated images\n            if (epoch + 1) % 25 == 0:\n                with torch.no_grad():\n                    latent_space = torch.randn(self.batch_size, self.z_dim).to(device)\n                    embedding_batch = high_test_embeds[0 : self.batch_size]\n                    embedding_tensor = torch.tensor(embedding_batch, dtype=torch.float32).to(device)\n                    embedding_batch_compressed = self.embedding_compressor(embedding_tensor).to(device)\n                    low_fake_images = self.stage1_generator(embedding_batch_compressed, latent_space)\n                    high_fake_images = self.stage2_generator(embedding_batch_compressed, low_fake_images)\n\n                    show_img_batch(low_fake_images, 64, (64, 64))\n                    show_img_batch(high_fake_images, 64, (128, 128))\n                    \n                    \"\"\"\n                    to_pil = ToPILImage()\n                    for i, image in enumerate(high_fake_images[:10]):\n                        pil_image = to_pil(image.cpu().detach())\n                        plt.imshow(pil_image)\n                        plt.show()\n                        pil_image.save(f'/kaggle/working/results_stage2/gen_{epoch + 1}_{i}.png')\n                    \"\"\"\n\n            # Save weights\n            if (epoch + 1) % 5 == 0:\n                torch.save(self.stage2_generator.state_dict(), f'/kaggle/working/results_stage2/stage2_gen_{epoch + 1}.pth')\n                torch.save(self.stage2_discriminator.state_dict(), f'/kaggle/working/results_stage2/stage2_disc_{epoch + 1}.pth')","metadata":{"execution":{"iopub.status.busy":"2023-10-13T20:43:13.353877Z","iopub.execute_input":"2023-10-13T20:43:13.354266Z","iopub.status.idle":"2023-10-13T20:43:13.381116Z","shell.execute_reply.started":"2023-10-13T20:43:13.354226Z","shell.execute_reply":"2023-10-13T20:43:13.380135Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"if (stage == 2):\n    generator = Stage1Generator()\n    checkpoint1 = torch.load('/kaggle/input/weights-gan1/stage1_gen_140.pth', map_location=device)\n    generator.load_state_dict(checkpoint1)\n\n    embedding_compressor = EmbeddingCompressor()\n    checkpoint2 = torch.load('/kaggle/input/weights-gan1/stage1_embco_140.pth', map_location=device)\n    embedding_compressor.load_state_dict(checkpoint2)\n\n    stage2_gen = Stage2Generator()\n    stage2_dis = Stage2Discriminator()\n\n    stackgan_stage2 = StackGanStage2(epochs=150)\n    stackgan_stage2.train_stage2()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Print of results","metadata":{}},{"cell_type":"code","source":"latent_space = torch.randn(64, 100).to(device)\nembedding_batch = test_embeds[0:64]\n\nstage1_generator = Stage1Generator().to(device)\nembedding_compressor = EmbeddingCompressor().to(device)\nstage2_generator = Stage2Generator().to(device).to(device)\n\ncheckpoint1 = torch.load('/kaggle/input/weightsss/stage1_gen_150.pth', map_location=device)\ncheckpoint2 = torch.load('/kaggle/input/weightsss/stage1_embco_150.pth', map_location=device)\ncheckpoint3 = torch.load('/kaggle/input/weightsss/stage2_gen_150.pth', map_location=device)\n\nstage1_generator.load_state_dict(checkpoint1)\nstage2_generator.load_state_dict(checkpoint3)\nembedding_compressor.load_state_dict(checkpoint2)\n\nlatent_space = torch.randn(64, 100).to(device)\nembedding_batch = test_embeds[0 : 64]\ncompressed_embedding_batch = embedding_compressor(torch.tensor(embedding_batch, dtype=torch.float32).to(device)).to(device)\ngen_images = stage1_generator(compressed_embedding_batch, latent_space)\nhigh_fake_images = stage2_generator(compressed_embedding_batch, gen_images)\n\n\nshow_img_batch(gen_images, 64, (64, 64))\nshow_img_batch(high_fake_images, 64, (128, 128))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\n!zip -r file.zip /kaggle/working\nFileLink(r'file.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}